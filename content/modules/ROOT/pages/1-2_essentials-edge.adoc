= Understanding the Essentials of Edge: Strengths and Challenges

Our solution is designed to run directly while the vehicle is operating within the plant, during real operational conditions. This means it operates at the edge, far from centralized data centers. Therefore, the corresponding infrastructure must be as lightweight as possible due to space limitations. For this reason, compact architectures such as SNO and Red Hat Device Edge have been used. 

When considering moving the entire AI/ML lifecycle to the edge, it's important to weigh the pros and cons derived from this action. On the positive side, bringing AI to the edge means that models are trained, deployed and managed closer to where the data is generated, which introduces several distinct advantages:

* *Data sovereignty*: Processing data at its point of origin ensures that it remains within the local environment, eliminating the need to transport it to an outside infrastructure.
* *Limited-to-no connectivity*: Because data processing doesn't rely on network connectivity, critical operations can continue functioning uninterrupted even during network outages or in disconnected environments.
* *Faster decision-making*: With no need to transmit data over the network, latency is reduced, allowing for quicker response times and near-real-time decision-making.

However, the edge environment presents challenges as well. Beyond the typical network connectivity limitations, there are other drawbacks to consider. As we move closer to the edge, the environment becomes more constrained. AI/ML processes often demand significant power and hardware resources, which can be difficult to provide in an environment with limited space and resources. Additionally, edge locations often lack extensive dedicated IT support, requiring architectures that are resilient and simple, with operations that are automated and need minimal human intervention.